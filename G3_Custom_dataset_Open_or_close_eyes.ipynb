{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snurfatin/HomeHeating/blob/main/G3_Custom_dataset_Open_or_close_eyes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu5EoMTZpGPe"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rokxI_qOpGPk"
      },
      "source": [
        "## Structure of our data folder\n",
        "\n",
        "For this exercise, we’ll keep the following folder structure:\n",
        "\n",
        "<div> <img src=\"https://github.com/CUTe-EmbeddedAI/DLIVA_CV_5Day_Workshop/blob/main/day3/images/fig47.png?raw=1\" alt=\"Drawing\" style=\"width: 300px;\"/></div> \n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig47.png)\n",
        "\n",
        "This is a straightforward folder structure with a root folder as the Train/Test folders containing classes with images inside them. \n",
        "\n",
        "*However, some other dataset, as you’ll see in the future, might have a slightly different structure. It doesn’t matter in what structure we get the data in. The data can all be in a single folder with class names in the image names (like “Cat_001.jpg”) or even in a CSV, we can process all this in our custom dataset class.\n",
        "\n",
        "Let's apply some transformations to our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Veui89ripGPp"
      },
      "outputs": [],
      "source": [
        "# Applying Transforms to the Data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "image_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoUb9dAOpGPq"
      },
      "source": [
        "## Method 1: Define dataset using `torchvision.datasets.ImageFolder`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGWbs8c-pGPr",
        "outputId": "26263d06-d928-4a1b-86fc-0d83d294dc1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "# Load the Data\n",
        "\n",
        "# Set train and valid directory paths\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "dataset = '/content/gdrive/My Drive/DL_open_or_close_eyes'\n",
        "\n",
        "train_directory = os.path.join(dataset, 'train')\n",
        "test_directory = os.path.join(dataset, 'validation')\n",
        "\n",
        "# Batch size\n",
        "batchSize = 32\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(os.listdir(train_directory))\n",
        "print(num_classes)\n",
        "\n",
        "# Load Data from folders\n",
        "data = {\n",
        "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
        "\n",
        "    'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'])\n",
        "}\n",
        "\n",
        "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
        "# idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
        "# print(idx_to_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JAEEe58pGPs"
      },
      "source": [
        "Let's see the info on train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4m0JrIXpGPt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f9df57-e6a3-476e-8b29-cfe55068a219"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 178\n",
              "    Root location: /content/gdrive/My Drive/DL_open_or_close_eyes/train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
              "               RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n",
              "               RandomHorizontalFlip(p=0.5)\n",
              "               CenterCrop(size=(224, 224))\n",
              "               ToTensor()\n",
              "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuZoH8SVpGPu"
      },
      "outputs": [],
      "source": [
        "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
        "train_data_size = len(data['train'])\n",
        "# valid_data_size = len(data['valid'])\n",
        "test_data_size = len(data['test'])\n",
        "\n",
        "# Create iterators for the Data loaded using DataLoader module\n",
        "trainloader = DataLoader(data['train'], batch_size=batchSize, shuffle=True)\n",
        "# valid_data_loader = DataLoader(data['valid'], batch_size=batchSize, shuffle=True)\n",
        "testloader = DataLoader(data['test'], batch_size=batchSize, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lllGMRytpGPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ced7e3-fabc-4322-bdf9-6192dc18d363"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178, 148)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_data_size, test_data_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pHq9YTbpGPw"
      },
      "outputs": [],
      "source": [
        "input_size = (3,32,32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTuCiNMepGPx"
      },
      "outputs": [],
      "source": [
        "# #######################################################\n",
        "# #                  Create Dataloader                     #\n",
        "# #######################################################\n",
        "\n",
        "# # Turn train and test custom Dataset's into DataLoader's\n",
        "# from torch.utils.data import DataLoader\n",
        "# trainloader = DataLoader(dataset=data['train'], # use custom created train Dataset\n",
        "#                                      batch_size=4, # how many samples per batch?\n",
        "#                                      num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
        "#                                      shuffle=True) # shuffle the data?\n",
        "\n",
        "# testloader = DataLoader(dataset=data['test'], # use custom created test Dataset\n",
        "#                                     batch_size=4, \n",
        "#                                     num_workers=0, \n",
        "#                                     shuffle=False) # don't usually need to shuffle testing data\n",
        "\n",
        "# train_data_size = len(trainloader.dataset)\n",
        "# test_data_size = len(testloader.dataset)\n",
        "\n",
        "# print(train_data_size)\n",
        "# print(test_data_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7TYXzQ4pGPy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f3c7544ca2834c5abc8dfbe14f639214",
            "825be49e46a748698131ac1af2d4a197",
            "079c0a44316d4ab2a4f18f65768a2faa",
            "7baea1b849354cfcbc8770c1070f4d41",
            "801888585d7f4f1b85fd1625687e02f1",
            "5b95e48bb6964da987185b66baaa5edb",
            "838ec41ab9204b5ba1a8229c424d1319",
            "2f52e7918fe945449ea8a1f06abfa9e1",
            "f6eeaa89ac11457081c45457017a52ff",
            "0835591dd55440d0b0fd03e4726f04e5",
            "cb92fa823d564c24a5db5c2c60bde8f2"
          ]
        },
        "outputId": "7779dede-6be8-4819-d43a-ffef8aff1460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3c7544ca2834c5abc8dfbe14f639214"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "#######################\n",
        "# DEFINE YOUR OWN MODEL\n",
        "\n",
        "model_ft = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#######################\n",
        "\n",
        "# 2. LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 3. move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model_ft.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEVE8du9pGPz"
      },
      "outputs": [],
      "source": [
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9tt7xW0pGP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d23431-163e-4254-aa84-85a255980845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 000, Training: Loss: 0.6619, Accuracy: 57.3034%, \n",
            "\t\tValidation : Loss : 0.5716, Accuracy: 68.2432%, Time: 125.1444s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.5161, Accuracy: 74.1573%, \n",
            "\t\tValidation : Loss : 0.4824, Accuracy: 78.3784%, Time: 60.0376s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.3443, Accuracy: 91.5730%, \n",
            "\t\tValidation : Loss : 0.4686, Accuracy: 77.0270%, Time: 60.0478s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.2204, Accuracy: 94.9438%, \n",
            "\t\tValidation : Loss : 0.4491, Accuracy: 80.4054%, Time: 59.8258s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.1549, Accuracy: 97.7528%, \n",
            "\t\tValidation : Loss : 0.4850, Accuracy: 77.7027%, Time: 59.9452s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.1344, Accuracy: 98.3146%, \n",
            "\t\tValidation : Loss : 0.6262, Accuracy: 75.6757%, Time: 59.8437s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.0984, Accuracy: 98.3146%, \n",
            "\t\tValidation : Loss : 0.6013, Accuracy: 75.6757%, Time: 59.9174s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.0644, Accuracy: 99.4382%, \n",
            "\t\tValidation : Loss : 0.5472, Accuracy: 74.3243%, Time: 59.7516s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.0548, Accuracy: 99.4382%, \n",
            "\t\tValidation : Loss : 0.5860, Accuracy: 74.3243%, Time: 59.7367s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.0421, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.6395, Accuracy: 74.3243%, Time: 59.8366s\n"
          ]
        }
      ],
      "source": [
        "# 4. Train the model for 10 epochs\n",
        " \n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model_ft, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL9VMzwc-6WV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "f806120c-46b9-4198-9fb7-36adcfacc498"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 5. Analyze the loss curve\n",
        "\n",
        "history = np.array(history)\n",
        "plt.plot(history[:,0:2])\n",
        "plt.legend(['Tr Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0,3)\n",
        "# plt.savefig('cifar10_loss_curve.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAGYVuf0-6WW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "81a141fc-b11b-4991-c379-b35ff4371523"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 6. Analyze the accuracy curve\n",
        "\n",
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "# plt.savefig('cifar10_accuracy_curve.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyBTMMjIpGP1"
      },
      "source": [
        "## Method 2: Define custom dataset in Pytorch\n",
        "\n",
        "Pytorch has a great ecosystem to load custom datasets for training machine learning models.\n",
        "\n",
        "This is a straightforward folder structure with a root folder as the Train/Test folders containing classes with images inside them. As we’ll see, it doesn’t matter in what structure we get the data in. The data can all be in a single folder with class names in the image names (like “Cat_001.jpg”) or even in a CSV, we can process all this in our custom dataset class.\n",
        "\n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig47.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD2vu2-BpGP2"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import cv2\n",
        "import glob\n",
        "import numpy\n",
        "import random\n",
        "from pandas.core.common import flatten\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlBTvIC7pGP2"
      },
      "outputs": [],
      "source": [
        "# Applying Transforms to the Data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "image_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbPvryT3pGP3"
      },
      "source": [
        "Next, we create the Train, Valid, and Test sets. Here we create separate lists of image paths for Train, Valid, and Test sets. These will be used in our Dataset class which will be defined for a custom dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSAxJ0sIlauo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "dataset = '/content/gdrive/My Drive/fruit_dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QyD3_gspGP3"
      },
      "outputs": [],
      "source": [
        "####################################################\n",
        "#       Create Train, Valid and Test sets\n",
        "####################################################\n",
        "train_data_path = dataset + '/train' \n",
        "test_data_path = dataset + '/validation'\n",
        "\n",
        "train_image_paths = [] #to store image paths in list\n",
        "classes = [] #to store class values\n",
        "\n",
        "#1.\n",
        "# get all the paths from train_data_path and append image paths and class to to respective lists\n",
        "\n",
        "for data_path in glob.glob(train_data_path + '/*'):\n",
        "    classes.append(data_path.split('/')[-1]) \n",
        "    train_image_paths.append(glob.glob(data_path + '/*'))\n",
        "    \n",
        "train_image_paths = list(flatten(train_image_paths))\n",
        "random.shuffle(train_image_paths)\n",
        "\n",
        "print('train_image_path example: ', train_image_paths[0])\n",
        "print('class example: ', classes[0])\n",
        "\n",
        "#2.\n",
        "# create the test_image_paths\n",
        "test_image_paths = []\n",
        "for data_path in glob.glob(test_data_path + '/*'):\n",
        "    test_image_paths.append(glob.glob(data_path + '/*'))\n",
        "\n",
        "test_image_paths = list(flatten(test_image_paths))\n",
        "\n",
        "print(\"Train size: {}\\nTest size: {}\".format(len(train_image_paths), len(test_image_paths)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WnFBuLIpGP5"
      },
      "source": [
        "We can’t use the class names directly for models. We create mappings of classes to index and index to classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pWX-w_OpGP5"
      },
      "outputs": [],
      "source": [
        "#######################################################\n",
        "#      Create dictionary for class indexes\n",
        "#######################################################\n",
        "\n",
        "idx_to_class = {i:j for i, j in enumerate(classes)}\n",
        "class_to_idx = {value:key for key,value in idx_to_class.items()}\n",
        "class_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ieQ94EppGP6"
      },
      "source": [
        "### Create Dataset Class\n",
        "\n",
        "This is the core of our custom dataset. The structure of the dataset class is something like this:\n",
        "\n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig48.png) \n",
        "\n",
        "We create our customDataset class by inheriting the Dataset class:\n",
        "\n",
        ">from torch.utils.data import Dataset\n",
        "\n",
        "First, we define the **init** function. As soon as we create an instance of our customDataset class, this function is called by default. This function should contain all operations that we want to run on the whole dataset (eg. train) once. For now, we define the variables for image_paths and transforms for the corresponding Train and Test sets.\n",
        "\n",
        "Then we have the **len** function which just returns the length of the dataset. This is used afterward by the DataLoader to create batches.\n",
        "\n",
        "And finally, we have **getitem**. This processes and returns 1 datapoint at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIA3jjz8pGP6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "#######################################################\n",
        "#               Define Dataset Class\n",
        "#######################################################\n",
        "\n",
        "class fruitDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=False):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filepath = self.image_paths[idx]\n",
        "#         image = cv2.imread(image_filepath)\n",
        "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.open(image_filepath) # if using torchvision transforms\n",
        "        # print(image_filepath)\n",
        "        label = image_filepath.split('/')[-2]\n",
        "        # print(label)\n",
        "        label = class_to_idx[label]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image) # if using torchvision transforms\n",
        "        \n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US85U4HbpGP7"
      },
      "source": [
        "As can be seen above, __getitem__ expects an index. This is handled automatically by the dataloader which for every image in the batch runs __getitem__. In the code for __getitem__, we load the image at index “idx”, extract the label from the file path and then run it through our defined transform. The function returns the Tensor of the image array and its corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwzBKz3jpGP7"
      },
      "outputs": [],
      "source": [
        "#######################################################\n",
        "#                  Create Dataset\n",
        "#######################################################\n",
        "\n",
        "train_dataset = fruitDataset(train_image_paths,image_transforms['train'])\n",
        "test_dataset = fruitDataset(test_image_paths,image_transforms['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbqYu16a-6WY"
      },
      "outputs": [],
      "source": [
        "train_dataset.transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfUCDyO1pGP7"
      },
      "source": [
        "After creating the train_dataset, we can access one example as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqZzTGeRpGP8"
      },
      "outputs": [],
      "source": [
        "print('The shape of tensor for 50th image in train dataset: ',train_dataset[49][0].shape)\n",
        "print('The label for 50th image in train dataset: ',train_dataset[49][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qApD_A6TpGP9"
      },
      "source": [
        "The final step. DataLoader class is used to load data in batches for the model. This helps us processing data in mini-batches that can fit within our GPU’s RAM. First, we import the DataLoader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10uAPyqbpGP9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1czDr28pGP9"
      },
      "source": [
        "Initiating the dataloader by sending in an object of the dataset and the batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_rTsHDupGP-"
      },
      "outputs": [],
      "source": [
        "#######################################################\n",
        "#                  Create Dataloader                     #\n",
        "#######################################################\n",
        "\n",
        "# Turn train and test custom Dataset's into DataLoader's\n",
        "from torch.utils.data import DataLoader\n",
        "trainloader = DataLoader(dataset=train_dataset, # use custom created train Dataset\n",
        "                                     batch_size=32, # how many samples per batch?\n",
        "                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
        "                                     shuffle=True) # shuffle the data?\n",
        "\n",
        "testloader = DataLoader(dataset=test_dataset, # use custom created test Dataset\n",
        "                                    batch_size=32, \n",
        "                                    num_workers=0, \n",
        "                                    shuffle=False) # don't usually need to shuffle testing data\n",
        "\n",
        "train_data_size = len(trainloader.dataset)\n",
        "test_data_size = len(testloader.dataset)\n",
        "\n",
        "print(train_data_size)\n",
        "print(test_data_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxnuI5HcpGP-"
      },
      "source": [
        "Once we have the dataloader instance — trainloader, we can use an iterator to access the data like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_HtQrflpGP-"
      },
      "outputs": [],
      "source": [
        "#batch of image tensor\n",
        "next(iter(trainloader))[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtNMfI8EpGP_"
      },
      "outputs": [],
      "source": [
        "#batch of the corresponding labels\n",
        "next(iter(trainloader))[1].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBhTZjI7pGQA"
      },
      "source": [
        "This is what we use to batch out the data in our training loop. Every time we run the iterator, the dataloader selects the next 64 indexes and runs it through the __getitem__ in dataset class one by one and then returns it to the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx0Es2OmpGQA"
      },
      "outputs": [],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model.fc = nn.Linear(num_ftrs, 4)\n",
        "\n",
        "# 2. LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 3. move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3A2QvpFn0lC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "\n",
        "model.to(device)\n",
        "summary(model, (3, 224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hj9UqJUapGQA"
      },
      "outputs": [],
      "source": [
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yQmzyDZpGQB"
      },
      "outputs": [],
      "source": [
        "# 4. Train the model for 10 epochs\n",
        " \n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90blSVXopGQC"
      },
      "outputs": [],
      "source": [
        "# 5. Analyze the loss curve\n",
        "\n",
        "history = np.array(history)\n",
        "plt.plot(history[:,0:2])\n",
        "plt.legend(['Tr Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0,3)\n",
        "# plt.savefig('cifar10_loss_curve.png')\n",
        "plt.show()\n",
        "\n",
        "# 6. Analyze the accuracy curve\n",
        "\n",
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "# plt.savefig('cifar10_accuracy_curve.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi1VFfSi-6Wa"
      },
      "source": [
        "## Inference on webcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBtn6j12-6Wa"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "\n",
        "\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxcWWjvT-6Wa"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0Lf_WcVIAdd"
      },
      "outputs": [],
      "source": [
        "transform=transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(size=224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPshNGU4Kfe4"
      },
      "outputs": [],
      "source": [
        "categories = ['thumbs_up','thumbs_down']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px0WWK9d-6Wb"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    frame = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    #     rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    # Apply transforms to the input image.\n",
        "    # input_tensor = transform(frame)\n",
        "    input_tensor = transform(frame)\n",
        "    # Add the batch dimension.\n",
        "    input_batch = input_tensor.unsqueeze(0)\n",
        "    input_batch = input_batch.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        output = model(input_batch)\n",
        "        end_time = time.time()\n",
        "    # Get the softmax probabilities.\n",
        "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "    # Check the top 5 categories that are predicted.\n",
        "    top5_prob, top5_catid = torch.topk(probabilities, 2)\n",
        "    \n",
        "    cv2.putText(frame, f\"{top5_prob[0].item()*100:.3f}%\", (15, (1)*30), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "    cv2.putText(frame, f\"{categories[top5_catid[0]]}\", (160, (1)*30), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "    print(categories[top5_catid[0]], top5_prob[0].item())\n",
        "    cv2_imshow(frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9soQE0QHsPB"
      },
      "outputs": [],
      "source": [
        "type(frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiiH1icYJ7Um"
      },
      "outputs": [],
      "source": [
        "frame.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZWq7dl_KCYG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "rokxI_qOpGPk"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.7 (pytorch_hasan)",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f3c7544ca2834c5abc8dfbe14f639214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_825be49e46a748698131ac1af2d4a197",
              "IPY_MODEL_079c0a44316d4ab2a4f18f65768a2faa",
              "IPY_MODEL_7baea1b849354cfcbc8770c1070f4d41"
            ],
            "layout": "IPY_MODEL_801888585d7f4f1b85fd1625687e02f1"
          }
        },
        "825be49e46a748698131ac1af2d4a197": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b95e48bb6964da987185b66baaa5edb",
            "placeholder": "​",
            "style": "IPY_MODEL_838ec41ab9204b5ba1a8229c424d1319",
            "value": "100%"
          }
        },
        "079c0a44316d4ab2a4f18f65768a2faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f52e7918fe945449ea8a1f06abfa9e1",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6eeaa89ac11457081c45457017a52ff",
            "value": 46830571
          }
        },
        "7baea1b849354cfcbc8770c1070f4d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0835591dd55440d0b0fd03e4726f04e5",
            "placeholder": "​",
            "style": "IPY_MODEL_cb92fa823d564c24a5db5c2c60bde8f2",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 175MB/s]"
          }
        },
        "801888585d7f4f1b85fd1625687e02f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b95e48bb6964da987185b66baaa5edb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "838ec41ab9204b5ba1a8229c424d1319": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f52e7918fe945449ea8a1f06abfa9e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6eeaa89ac11457081c45457017a52ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0835591dd55440d0b0fd03e4726f04e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb92fa823d564c24a5db5c2c60bde8f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}